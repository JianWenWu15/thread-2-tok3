import praw  # Python Reddit Wrapper
import edge_tts  # Better TTS
import asyncio
from moviepy.editor import VideoFileClip, AudioFileClip, CompositeVideoClip, ImageClip, CompositeAudioClip, TextClip
from pydub import AudioSegment
from dotenv import load_dotenv
import os
import random
import re
import textwrap
from PIL import Image, ImageDraw, ImageFont
import numpy as np

# Load environment variables
load_dotenv()

# Reddit API setup
reddit = praw.Reddit(
    client_id=os.getenv("CLIENT_ID"),
    client_secret=os.getenv("CLIENT_SECRET"),
    user_agent="thread-2-tok/0.1 by u/Complex_Balance4016"
)

# Helper function to fetch a Reddit story
def fetch_story(subreddit="AmItheAsshole"):
    """Fetch a random story from a subreddit."""
    subreddit = reddit.subreddit(subreddit)
    posts = [post for post in subreddit.hot(limit=10) if post.selftext]
    if posts:
        selected_post = random.choice(posts)
        return {
            "title": selected_post.title,
            "body": selected_post.selftext
        }
    return None

# Helper function to clean and process text
def process_text(text):
    """Clean and format text for better readability and TTS."""
    # Remove markdown formatting
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # Remove bold
    text = re.sub(r'\*(.*?)\*', r'\1', text)      # Remove italic
    text = re.sub(r'~~(.*?)~~', r'\1', text)      # Remove strikethrough
    
    # Clean up common Reddit formatting
    text = re.sub(r'\n+', ' ', text)              # Replace multiple newlines with space
    text = re.sub(r'\s+', ' ', text)              # Replace multiple spaces with single space
    text = text.strip()
    
    # Limit text length for reasonable video duration (adjust as needed)
    if len(text) > 1500:
        text = text[:1500] + "..."
    
    return text

# Helper function to generate better quality narration
async def generate_narration_async(text, output_file="narration.wav", voice="en-US-AriaNeural"):
    """Generate high-quality audio from text using Edge TTS."""
    try:
        output_path = os.path.join(os.getcwd(), output_file)
        
        # Available voices: en-US-AriaNeural (female), en-US-GuyNeural (male), 
        # en-US-JennyNeural (female), en-US-DavisNeural (male)
        communicate = edge_tts.Communicate(text, voice)
        await communicate.save(output_path)
        
        return output_path
    except Exception as e:
        print(f"Error generating narration: {e}")
        return None

def generate_narration(text, output_file="narration.wav", voice="en-US-AriaNeural"):
    """Wrapper for async narration generation."""
    return asyncio.run(generate_narration_async(text, output_file, voice))

# Helper function to create text image using PIL with better fonts
# Optimized helper function to create text images with pre-loaded font
def create_optimized_text_image(text, font_obj, width=1100, height=220):
    """Creates an optimized text image using a pre-loaded font object for efficiency."""
    try:
        print(f"ðŸš€ Creating optimized text image for: {text[:20]}...")
        
        # Use 2x resolution for anti-aliasing (quality optimization)
        scale_factor = 2
        high_width = width * scale_factor
        high_height = height * scale_factor
        
        # Create high-resolution image with transparency
        img = Image.new('RGBA', (high_width, high_height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        # Split text into lines and calculate layout
        lines = text.split('\n')
        print(f"ðŸ“ Text split into {len(lines)} lines")
        
        # Use pre-loaded font (major performance improvement)
        font = font_obj
        
        # Calculate text dimensions and positioning
        line_heights = []
        max_line_width = 0
        
        for line in lines:
            bbox = draw.textbbox((0, 0), line, font=font)
            line_height = bbox[3] - bbox[1]
            line_width = bbox[2] - bbox[0]
            line_heights.append(line_height)
            max_line_width = max(max_line_width, line_width)
        
        total_text_height = sum(line_heights) + (len(lines) - 1) * (scale_factor * 10)
        start_y = (high_height - total_text_height) // 2
        line_spacing = scale_factor * 10
        current_y = start_y
        
        # Render text with optimized styling
        for i, line in enumerate(lines):
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
            x = (high_width - text_width) // 2
            
            # Efficient shadow and outline rendering
            shadow_offset = 4 * scale_factor
            outline_width = 6 * scale_factor
            
            # Single shadow layer for efficiency
            draw.text((x + shadow_offset, current_y + shadow_offset), line, font=font, fill=(0, 0, 0, 120))
            
            # Simplified outline for performance
            for adj in range(-outline_width, outline_width + 1, 2):  # Step by 2 for efficiency
                for adj2 in range(-outline_width, outline_width + 1, 2):
                    if adj != 0 or adj2 != 0:
                        draw.text((x + adj, current_y + adj2), line, font=font, fill=(0, 0, 0, 200))
            
            # Main text
            draw.text((x, current_y), line, font=font, fill=(255, 255, 245, 255))
            current_y += line_heights[i] + line_spacing
        
        # Resize with high-quality resampling
        img = img.resize((width, height), Image.LANCZOS)
        
        # Convert to numpy array
        result = np.array(img)
        return result
        
    except Exception as e:
        print(f"Error creating optimized text image: {e}")
        return None

# Helper function to create text images with better font support
def create_text_image(text, width=1100, height=220, font_size=64):
    """Create a high-quality text image using PIL with modern styling."""
    try:
        print(f"ðŸ“ Creating text image for: {text[:50]}...")
        
        # Create image with higher resolution and transparent background
        # Use 2x resolution for better quality, then scale down for anti-aliasing
        scale_factor = 2
        high_width = width * scale_factor
        high_height = height * scale_factor
        high_font_size = font_size * scale_factor
        
        img = Image.new('RGBA', (high_width, high_height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        # Try to use modern fonts first, then fallback to system fonts
        fonts_dir = os.path.join(os.getcwd(), "fonts")
        font_paths = [
            # Local downloaded fonts
            os.path.join(fonts_dir, "Poppins-Bold.ttf"),
            os.path.join(fonts_dir, "Oswald-Bold.ttf"),
            os.path.join(fonts_dir, "Montserrat-Bold.ttf"),
            os.path.join(fonts_dir, "Inter-Bold.ttf"),
            # System fonts with good readability (bold variants)
            "C:/Windows/Fonts/impact.ttf",
            "C:/Windows/Fonts/arialbd.ttf",
            "C:/Windows/Fonts/trebucbd.ttf",  # Trebuchet MS Bold
            "C:/Windows/Fonts/calibrib.ttf",
            "C:/Windows/Fonts/frankgot.ttf",  # Franklin Gothic
            "C:/Windows/Fonts/segoeuib.ttf",  # Segoe UI Bold
            "C:/Windows/Fonts/verdanab.ttf",
            # Regular fallbacks
            "C:/Windows/Fonts/arial.ttf",
            "arial.ttf"
        ]
        
        font = None
        for font_path in font_paths:
            try:
                font = ImageFont.truetype(font_path, high_font_size)
                print(f"âœ… Using font: {font_path}")
                break
            except Exception as font_error:
                continue
        
        if font is None:
            font = ImageFont.load_default()
            print("âš ï¸ Using default font")
        
        # Split text into lines and measure text properly
        lines = text.split('\n')
        print(f"ðŸ“ Text split into {len(lines)} lines")
        
        # Auto-wrap long lines to prevent cutoff with better logic
        max_chars_per_line = 18  # Reduced for better mobile viewing
        wrapped_lines = []
        for line in lines:
            if len(line) > max_chars_per_line:
                words = line.split()
                current_line = ""
                for word in words:
                    # Check if adding the word would exceed the limit
                    test_line = current_line + (" " if current_line else "") + word
                    
                    # Use actual text width measurement instead of character count
                    bbox = draw.textbbox((0, 0), test_line, font=font or ImageFont.load_default())
                    test_width = bbox[2] - bbox[0]
                    max_width = high_width * 0.85  # Use 85% of width for safety margin
                    
                    if test_width <= max_width and len(test_line) <= max_chars_per_line:
                        current_line = test_line
                    else:
                        if current_line:
                            wrapped_lines.append(current_line)
                        current_line = word
                if current_line:
                    wrapped_lines.append(current_line)
            else:
                wrapped_lines.append(line)
        
        lines = wrapped_lines
        print(f"ðŸ“ Text wrapped into {len(lines)} lines")
        
        # Calculate total text dimensions
        line_heights = []
        max_line_width = 0
        line_spacing = int(15 * scale_factor)
        
        for line in lines:
            bbox = draw.textbbox((0, 0), line, font=font)
            line_height = bbox[3] - bbox[1]
            line_width = bbox[2] - bbox[0]
            line_heights.append(line_height)
            max_line_width = max(max_line_width, line_width)
        
        total_text_height = sum(line_heights) + (line_spacing * (len(lines) - 1))
        
        # Center the text block vertically
        y_start = (high_height - total_text_height) // 2
        
        # Draw each line with modern styling
        current_y = y_start
        for i, line in enumerate(lines):
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
            x = (high_width - text_width) // 2  # Center horizontally
            
            # Draw modern text styling with improved shadow and outline
            shadow_offset = 4 * scale_factor  # Slightly larger shadow
            outline_width = 6 * scale_factor   # Thicker outline for better readability
            
            # Draw multiple shadow layers for depth
            for shadow_layer in range(2):
                offset = shadow_offset + (shadow_layer * 2)
                alpha = 120 - (shadow_layer * 40)  # Fading shadow layers
                draw.text((x + offset, current_y + offset), line, font=font, fill=(0, 0, 0, alpha))
            
            # Draw thick black outline with gradient effect
            for adj in range(-outline_width, outline_width + 1):
                for adj2 in range(-outline_width, outline_width + 1):
                    if adj != 0 or adj2 != 0:
                        # Stronger outline at the edges
                        distance = (adj**2 + adj2**2)**0.5
                        alpha = max(200, 255 - int(distance * 10))
                        draw.text((x + adj, current_y + adj2), line, font=font, fill=(0, 0, 0, alpha))
            
            # Draw main text with slight yellow tint for warmth
            draw.text((x, current_y), line, font=font, fill=(255, 255, 245, 255))
            
            current_y += line_heights[i] + line_spacing
        
        # Scale down for anti-aliasing effect
        print(f"ðŸ“ Resizing image from {img.size} to ({width}, {height})")
        img = img.resize((width, height), Image.LANCZOS)
        print("âœ… Image resized successfully")
        
        # Convert to numpy array for MoviePy
        print("ðŸ”„ Converting to numpy array...")
        result = np.array(img)
        print(f"âœ… Text image created successfully: {result.shape}")
        return result
        
    except Exception as e:
        print(f"Error creating text image: {e}")
        return None

# Helper function to estimate speech timing
def estimate_speech_timing(text, total_duration):
    """Estimate when each word will be spoken based on average speech rate."""
    words = text.split()
    # More realistic speech rate: ~120 words per minute = 2.0 words per second
    # This accounts for natural pauses and slower TTS speech
    estimated_speech_rate = 2.0  # words per second (reduced from 2.5)
    
    # Calculate timing for each word
    word_timings = []
    current_time = 0
    
    for word in words:
        # More conservative timing with better punctuation handling
        base_duration = len(word) * 0.12  # Increased from 0.08 to 0.12
        
        # Add extra time for punctuation and longer words
        if word.endswith(('.', '!', '?')):
            base_duration += 0.3  # Pause after sentences
        elif word.endswith(','):
            base_duration += 0.15  # Shorter pause after commas
        
        word_duration = max(0.5, base_duration)  # Minimum increased from 0.4s to 0.5s
        word_timings.append({
            'word': word,
            'start': current_time,
            'end': current_time + word_duration
        })
        current_time += word_duration
    
    # Scale to fit actual audio duration
    if word_timings:
        total_estimated_time = word_timings[-1]['end']
        time_scale = total_duration / total_estimated_time
        
        for timing in word_timings:
            timing['start'] *= time_scale
            timing['end'] *= time_scale
    
    return word_timings

# Helper function to create synchronized text segments
def create_synchronized_text_segments(text, audio_duration, words_per_segment=3):
    """Create text segments that are synchronized with speech timing."""
    word_timings = estimate_speech_timing(text, audio_duration)
    segments = []
    
    # Group words into segments with proper non-overlapping timing
    for i in range(0, len(word_timings), words_per_segment):
        segment_words = word_timings[i:i + words_per_segment]
        if segment_words:
            segment_text = ' '.join([word['word'] for word in segment_words])
            original_start = segment_words[0]['start']
            original_end = segment_words[-1]['end']
            
            # Add a small buffer to ensure text appears before speech starts
            segment_start = max(0, original_start - 0.2)  # Show text 0.2s early
            
            # Calculate duration based on the original timing to prevent overlap
            segment_duration = original_end - original_start + 0.2  # Include the early start buffer
            
            # Ensure segments don't overlap by checking against previous segment
            if segments:
                previous_segment = segments[-1]
                previous_end = previous_segment['start'] + previous_segment['duration']
                if segment_start < previous_end:
                    # Adjust start time to prevent overlap
                    segment_start = previous_end
                    # Recalculate duration to maintain original end time
                    segment_duration = max(0.5, original_end - segment_start)
            
            segments.append({
                'text': segment_text,
                'start': segment_start,
                'duration': segment_duration
            })
    
    return segments

# Helper function to create text segments for video overlay
def create_text_segments(text, words_per_segment=8):
    """Split text into segments for display during video."""
    words = text.split()
    segments = []
    
    for i in range(0, len(words), words_per_segment):
        segment = ' '.join(words[i:i + words_per_segment])
        segments.append(segment)
    
    return segments

# Helper function to add background music
def add_background_music(video_path, music_folder="static/music", volume=0.07):
    """Add background music to video if music files are available."""
    try:
        if not os.path.exists(music_folder):
            return video_path
        
        music_files = [f for f in os.listdir(music_folder) if f.endswith(('.mp3', '.wav', '.m4a'))]
        if not music_files:
            return video_path
        
        # Select random background music
        selected_music = random.choice(music_files)
        music_path = os.path.join(music_folder, selected_music)
        
        video = VideoFileClip(video_path)
        background_music = AudioFileClip(music_path)
        
        # Loop music if it's shorter than video
        if background_music.duration < video.duration:
            background_music = background_music.loop(duration=video.duration)
        else:
            background_music = background_music.subclip(0, video.duration)
        
        # Lower the volume of background music
        background_music = background_music.volumex(volume)
        
        # Mix original audio with background music
        final_audio = CompositeAudioClip([video.audio, background_music])
        final_video = video.set_audio(final_audio)
        
        # Save with background music using high quality settings
        output_with_music = video_path.replace('.mp4', '_with_music.mp4')
        final_video.write_videofile(
            output_with_music, 
            codec="libx264", 
            audio_codec="aac",
            fps=30,  # Higher fps for smoother video
            bitrate="8000k",  # High bitrate for better quality
            ffmpeg_params=[
                "-crf", "18",  # High quality encoding
                "-preset", "slow",  # Better compression
                "-profile:v", "high",
                "-level", "4.0",
                "-pix_fmt", "yuv420p"
            ]
        )
        
        return output_with_music
    
    except Exception as e:
        print(f"Error adding background music: {e}")
        return video_path

# Helper function to create enhanced video with text overlays
def create_video_with_text(input_video_file, input_audio_file, text, output_file):
    """Creates a TikTok-style video with text overlays and audio."""
    try:
        output_path = os.path.join(os.getcwd(), output_file)

        # Load video and audio
        video = VideoFileClip(input_video_file)
        audio = AudioFileClip(input_audio_file)

        # Calculate audio duration and select a video slice
        audio_duration = audio.duration
        max_start_time = max(0, video.duration - audio_duration)
        start_time = random.uniform(0, max_start_time)
        end_time = start_time + audio_duration
        video_slice = video.subclip(start_time, end_time)

        # Crop video to fit TikTok's 9:16 aspect ratio with quality preservation
        target_aspect_ratio = 9 / 16
        video_width, video_height = video_slice.size
        current_aspect_ratio = video_width / video_height

        if current_aspect_ratio > target_aspect_ratio:
            # Crop width (landscape video) - ensure we maintain resolution
            new_width = int(video_height * target_aspect_ratio)
            # Ensure width is even for FFmpeg compatibility
            if new_width % 2 != 0:
                new_width -= 1
            crop_x1 = (video_width - new_width) // 2
            crop_x2 = crop_x1 + new_width
            video_cropped = video_slice.crop(x1=crop_x1, x2=crop_x2)
        else:
            # Crop height (portrait video) - ensure we maintain resolution  
            new_height = int(video_width / target_aspect_ratio)
            # Ensure height is even for FFmpeg compatibility
            if new_height % 2 != 0:
                new_height -= 1
            crop_y1 = (video_height - new_height) // 2
            crop_y2 = crop_y1 + new_height
            video_cropped = video_slice.crop(y1=crop_y1, y2=crop_y2)
        
        # Note: Removed upscaling to avoid PIL compatibility issues
        # The video quality will be preserved at its original resolution
        print(f"ðŸ“ Video dimensions: {video_cropped.size[0]}x{video_cropped.size[1]}")

        # Efficient text overlay creation with font pre-loading
        try:
            print("ðŸŽ¨ Creating synchronized text overlays with optimized rendering...")
            
            # Create synchronized text segments
            text_segments = create_synchronized_text_segments(text, audio_duration, words_per_segment=3)
            
            # Pre-load font ONCE for efficiency
            fonts_dir = os.path.join(os.getcwd(), "fonts")
            font_paths = [
                os.path.join(fonts_dir, "Poppins-Bold.ttf"),
                os.path.join(fonts_dir, "Oswald-Bold.ttf"),
                os.path.join(fonts_dir, "Montserrat-Bold.ttf"),
                "C:/Windows/Fonts/impact.ttf",
                "C:/Windows/Fonts/arialbd.ttf",
            ]
            
            selected_font_path = None
            for font_path in font_paths:
                if os.path.exists(font_path):
                    selected_font_path = font_path
                    print(f"âœ… Using font: {font_path}")
                    break
            
            if not selected_font_path:
                print("âš ï¸ No custom fonts found, using system default")
                selected_font_path = "arial.ttf"
            
            # Pre-load the font object ONCE (major optimization)
            try:
                font_obj = ImageFont.truetype(selected_font_path, size=128)  # 2x size for quality
                print(f"âœ… Font pre-loaded: {selected_font_path}")
            except Exception as font_error:
                print(f"âš ï¸ Font loading failed: {font_error}")
                font_obj = ImageFont.load_default()
            
            # Create text clips with optimized approach
            text_clips = []
            print(f"ðŸ“Š Processing {len(text_segments)} text segments...")
            
            for i, segment in enumerate(text_segments):
                if i % 20 == 0:  # Progress updates every 20 segments
                    print(f"ðŸ“ Processing segment {i+1}/{len(text_segments)}")
                
                # Smart text wrapping
                words = segment['text'].split()
                if len(words) > 4:
                    if len(words) <= 6:
                        mid_point = len(words) // 2
                        wrapped_text = ' '.join(words[:mid_point]) + '\n' + ' '.join(words[mid_point:])
                    else:
                        third = len(words) // 3
                        wrapped_text = ' '.join(words[:third]) + '\n' + ' '.join(words[third:third*2]) + '\n' + ' '.join(words[third*2:])
                elif len(words) in [3, 4]:
                    mid_point = len(words) // 2
                    wrapped_text = ' '.join(words[:mid_point]) + '\n' + ' '.join(words[mid_point:])
                else:
                    wrapped_text = segment['text']
                
                # Create optimized text image with pre-loaded font
                text_img = create_optimized_text_image(wrapped_text, font_obj, width=1100, height=220)
                
                if text_img is not None:
                    text_clip = (ImageClip(text_img, transparent=True)
                               .set_duration(segment['duration'])
                               .set_start(segment['start'])
                               .set_position('center'))
                    text_clips.append(text_clip)

            # Composite video with synchronized text overlays
            if text_clips:
                final_video = CompositeVideoClip([video_cropped] + text_clips)
                print(f"âœ… {len(text_clips)} synchronized text overlays created successfully!")
            else:
                final_video = video_cropped
                print("âš ï¸  No text overlays created, using video without text")
            
        except Exception as text_error:
            print(f"âš ï¸  Text overlay failed: {text_error}")
            print("ðŸ“ Continuing without text overlays...")
            final_video = video_cropped
        
        # Add audio to the video
        video_with_audio = final_video.set_audio(audio)

        # Write the final video with higher quality settings
        video_with_audio.write_videofile(
            output_path,
            codec="libx264",
            audio_codec="aac",
            temp_audiofile="temp-audio.m4a",
            remove_temp=True,
            fps=30,  # Increased from 24 to 30 for smoother video
            bitrate="8000k",  # High bitrate for better quality
            ffmpeg_params=[
                "-crf", "18",  # Lower CRF = higher quality (18 is high quality)
                "-preset", "slow",  # Slower preset = better compression efficiency
                "-profile:v", "high",  # High profile for better quality
                "-level", "4.0",  # H.264 level for good compatibility
                "-pix_fmt", "yuv420p"  # Standard pixel format for compatibility
            ]
        )

        # Clean up clips
        video.close()
        audio.close()
        video_with_audio.close()
        if 'final_video' in locals():
            final_video.close()

        return output_path if os.path.exists(output_path) else None
    except Exception as e:
        print(f"Error creating video: {e}")
        return None

if __name__ == "__main__":
    print("ðŸŽ¬ Enhanced Reddit Story to Video Generator")
    print("Fetching a story from Reddit and generating video with text overlays and background music...")

    # Fetch a story from the specified subreddit
    subreddit_name = "AmItheAsshole"
    story = fetch_story(subreddit_name)

    if story:
        print(f"âœ… Story fetched from r/{subreddit_name}")
        print(f"Title: {story['title'][:100]}...")
        
        # Process and clean the text
        raw_text = f"{story['title']} {story['body']}"
        processed_text = process_text(raw_text)
        print(f"ðŸ“ Text processed (length: {len(processed_text)} characters)")

        # File paths
        input_video = os.path.join(os.getcwd(), "static/minecraft_background.mp4")
        input_audio = os.path.join(os.getcwd(), "narration.wav")  # Changed to WAV for better quality
        output_video = os.path.join(os.getcwd(), "generated_video.mp4")

        # Generate high-quality narration audio
        print("ðŸŽ¤ Generating high-quality narration...")
        narration_path = generate_narration(processed_text, input_audio, voice="en-US-AriaNeural")

        if narration_path and os.path.exists(input_video):
            print("ðŸŽ¬ Creating video with text overlays...")
            video_path = create_video_with_text(input_video, input_audio, processed_text, output_video)
            
            if video_path:
                print(f"âœ… Base video created: {video_path}")
                
                # Add background music if available
                print("ðŸŽµ Adding background music...")
                final_video_path = add_background_music(video_path)
                print(f"ðŸŽ‰ Final video created: {final_video_path}")
                
                # Print summary
                print("\n" + "="*50)
                print("VIDEO GENERATION COMPLETE!")
                print("="*50)
                print(f"ðŸ“ Output file: {final_video_path}")
                print(f"ðŸ“Š Original text length: {len(raw_text)} characters")
                print(f"ðŸ“Š Processed text length: {len(processed_text)} characters")
                print(f"ðŸŽµ Background music: {'Added' if final_video_path != video_path else 'Not added (no music files found)'}")
                print("="*50)
            else:
                print("âŒ Error: Video generation failed.")
        else:
            print("âŒ Error: Input video or narration file not found.")
            if not os.path.exists(input_video):
                print(f"   Missing: {input_video}")
            if not narration_path:
                print("   Failed to generate narration audio")
    else:
        print(f"âŒ No stories found in the subreddit 'r/{subreddit_name}'.")